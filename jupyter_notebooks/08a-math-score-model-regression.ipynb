{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Math Score model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Train and evaluate a model for predicting a student's math score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- outputs/datasets/filtered/math-score-dataset.csv\n",
    "- outputs/datasets/split/math/math-test-score.csv\n",
    "- outputs/datasets/split/math/math-train-score.csv\n",
    "- outputs/datasets/split/math/math-test-vars.csv\n",
    "- outputs/datasets/split/math/math-train-vars.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- A pipeline to predict a student's math score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this notebook exists in the jupyter_notebooks directory, we need to change the current working directory from the jupyter_notebooks directory to the workspace, so that any directories created in further codes cells are added in the correct place. \n",
    "\n",
    "We access the current directory with the OS packages' `getcwd()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "current_directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to set the working directory as the parent of the current working directory, jupyter_notebooks\n",
    "\n",
    "- The `os.path.dirname()` method gets the parent directory\n",
    "- The `os.chir()` method defines the new current directory\n",
    "- We do this to access all of the project's files and directories, rather than those in the jupyter_notebooks directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_directory))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make certain of things, we now use a code cell to confirm that we have set the current working directory properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "### Feature Engineering\n",
    "from feature_engine.encoding import OrdinalEncoder\n",
    "\n",
    "### Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "### Feature  Selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "### ML algorithms \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math Score pipeline\n",
    "\n",
    "Considering that the math_score variable is a continuous numerical value, our objective in this notebook is to construct a pipeline for a regression model. We have previously noted only weak correlation levels between the categorical feature variables and the numerical target variables, so it is entirely possible that a regression model will fail to perform well enough to fulfil the business requirements. If that is the case, we will switch to using a classification model, using bins. \n",
    "\n",
    "First of all, let's load the required datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maths = pd.read_csv('outputs/datasets/filtered/math-score-dataset.csv')\n",
    "df_maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the train and test sets. We use the `squeeze()` method to transform the dataframe into a Series, as this is what the math_train_score and math_test_score variables were initially produced as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_train_vars = pd.read_csv('outputs/datasets/split/math/math-train-vars.csv')\n",
    "math_train_score = pd.read_csv('outputs/datasets/split/math/math-train-score.csv').squeeze()\n",
    "math_test_vars = pd.read_csv('outputs/datasets/split/math/math-test-vars.csv')\n",
    "math_test_score = pd.read_csv('outputs/datasets/split/math/math-test-score.csv').squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "We will now define the pipeline that we will use. We know that our dataset's feature variables are categorical, so we need an ordinal categorical encoder step first. As we saw in the 07-FeatureEngineering notebook, none of the variables are sufficiently correlated to warrant a smart correlated selection step. We will then use a standard feature scaling and feature selection steps, and finally have the model step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PipelineOptimization(model):\n",
    "  pipeline_base = Pipeline([\n",
    "\n",
    "      (\"OrdinalCategoricalEncoder\", OrdinalEncoder(encoding_method='arbitrary', \n",
    "                                                  variables = ['gender', 'ethnicity', 'parental_education', 'lunch_program', 'test_preparation_course'] ) ),\n",
    "       \n",
    "\n",
    "      (\"feature_scaling\", StandardScaler() ),\n",
    "\n",
    "      (\"feature_selection\",  SelectFromModel(model) ),\n",
    "\n",
    "      (\"model\", model ),\n",
    "       \n",
    "    ])\n",
    "\n",
    "  return pipeline_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm and Hyperparameter optimisation\n",
    "\n",
    "We now need to determine the best algorithm to use, and then for that algorithm, the best hyperparameters. To do this, we will use the custom class defined in the [6 - Modeling and Evaluation - predict Tenure notebook of the Churnometer walkthrough project.](https://github.com/AdamBoley/churnometer/blob/main/jupyter_notebooks/06%20-%20Modeling%20and%20Evaluation%20-%20Predict%20Tenure.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "class HyperparameterOptimizationSearch:\n",
    "\n",
    "    def __init__(self, models, parameters):\n",
    "        self.models = models\n",
    "        self.parameters = parameters\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    def fit(self, x, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
    "        for key in self.keys:\n",
    "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
    "            model =  PipelineOptimization(self.models[key])\n",
    "\n",
    "            parameters = self.parameters[key]\n",
    "            grid_search = GridSearchCV(model, parameters, cv=cv, n_jobs=n_jobs, verbose=verbose, scoring=scoring)\n",
    "            grid_search.fit(x, y)\n",
    "            self.grid_searches[key] = grid_search\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, parameters):\n",
    "            summary = {\n",
    "                 'estimator': key,\n",
    "                 'minimum_score': min(scores),\n",
    "                 'maximum_score': max(scores),\n",
    "                 'mean_score': np.mean(scores),\n",
    "                 'standard_deviation_score': np.std(scores),\n",
    "            }\n",
    "            return pd.Series({**parameters,**summary})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            parameters = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                result = self.grid_searches[k].cv_results_[key]        \n",
    "                scores.append(result.reshape(len(parameters), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(parameters, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'minimum_score', 'mean_score', 'maximum_score', 'standard_deviation_score']\n",
    "        columns = columns + [column for column in df.columns if column not in columns]\n",
    "\n",
    "        return df[columns], self.grid_searches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now select the algorithms to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_quick_search = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=0),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=0),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor(random_state=0),\n",
    "    \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0),\n",
    "    \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n",
    "    \"XGBRegressor\": XGBRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "parameters_quick_search = {\n",
    "    'LinearRegression': {},\n",
    "    \"DecisionTreeRegressor\": {},\n",
    "    \"RandomForestRegressor\": {},\n",
    "    \"ExtraTreesRegressor\": {},\n",
    "    \"AdaBoostRegressor\": {},\n",
    "    \"GradientBoostingRegressor\": {},\n",
    "    \"XGBRegressor\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the custom function using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_search = HyperparameterOptimizationSearch(models=models_quick_search, parameters=parameters_quick_search)\n",
    "model_search.fit(math_train_vars, math_train_score, scoring='r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check which algorithm performed best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search_summary, model_grid_search_pipelines = model_search.score_summary(sort_by='mean_score')\n",
    "model_grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the grid_search_summary dataframe, none of the algorithms perform particularly well, but the LinearRegression model performs best. Per the [SKlearn LinearRegression page](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?highlight=linearregression#sklearn.linear_model.LinearRegression), this algorithm has no hyperparameters that we can use to optimise performance.\n",
    "\n",
    "However, the RandomForestRegressor performed almost as well - it has a higher maximum score, but the mean score is brought down by the lower minimum score. The RandomForestRegressor has 5 hyperparameters that we can tweak, so we should see if these can boost performance to acceptable levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_search = {\n",
    "    \"RandomForestRegressor\":RandomForestRegressor(random_state=0),\n",
    "}\n",
    "\n",
    "# Hyperparamters:\n",
    "# https://docs.google.com/document/d/1PSug1nGF24NQHBByUzVFiNNKwvCMPLCc5x-ODfla28E/edit#\n",
    "\n",
    "parameters_search = {\n",
    "    \"RandomForestRegressor\":{'model__n_estimators': [100, 50, 140],\n",
    "                             'model__max_depth': [None, 4, 15],\n",
    "                             'model__min_samples_split': [2, 50],\n",
    "                             'model__min_samples_leaf': [1, 50],\n",
    "                             'model__max_leaf_nodes': [None, 50],\n",
    "                            }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_search = HyperparameterOptimizationSearch(models=models_search, parameters=parameters_search)\n",
    "param_search.fit(math_train_vars, math_train_score, scoring = 'r2', n_jobs=-1, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_search_summary, param_grid_search_pipelines = param_search.score_summary(sort_by='mean_score')\n",
    "param_grid_search_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, it looks like optimising the hyperparameters has failed to produce a model of adequate performance. Let's check the best performing models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model_grid_search_summary.iloc[0,0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_and_params = param_grid_search_summary.iloc[0,0]\n",
    "best_model_and_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the parameters for those models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search_pipelines[best_model].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_search_pipelines[best_model_and_params].best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the best pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_pipeline_1 = model_grid_search_pipelines[best_model].best_estimator_\n",
    "best_regressor_pipeline_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_pipeline_2 = param_grid_search_pipelines[best_model_and_params].best_estimator_\n",
    "best_regressor_pipeline_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assess which feature variables are the most important. Note that we can only do this for the RandomForestRegressor algorithm (best_regressor_pipeline_2), since the LinearRegression algorithm (best_regressor_pipeline_1) has no feature_importances_ method. Running the code cell below with the LinearRegression pipeline throws and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after data cleaning and feat engine, the feature may space changes\n",
    "data_cleaning_feat_eng_steps = 1 # how many data cleaning and feature engineering does your pipeline have?\n",
    "columns_after_data_cleaning_feat_eng = (Pipeline(best_regressor_pipeline_2.steps[:data_cleaning_feat_eng_steps])\n",
    "                                        .transform(math_train_vars)\n",
    "                                        .columns)\n",
    "\n",
    "best_features = columns_after_data_cleaning_feat_eng[best_regressor_pipeline_2['feature_selection'].get_support()].to_list()\n",
    "\n",
    "# create DataFrame to display feature importance\n",
    "df_feature_importance = (pd.DataFrame(data={\n",
    "          'Feature': columns_after_data_cleaning_feat_eng[best_regressor_pipeline_2['feature_selection'].get_support()],\n",
    "          'Importance': best_regressor_pipeline_2['model'].feature_importances_})\n",
    "  .sort_values(by='Importance', ascending=False)\n",
    "  )\n",
    "\n",
    "# Most important features statement and plot\n",
    "print(f\"* These are the {len(best_features)} most important features in descending order. \"\n",
    "      f\"The model was trained on them: \\n{df_feature_importance['Feature'].to_list()}\")\n",
    "\n",
    "df_feature_importance.plot(kind='bar',x='Feature',y='Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to support the conclusions we drew in the data analysis notebooks - the lunch_program variable has considerable importance, as to ethnicity and parental_education.\n",
    "\n",
    "We can also evaluate the performance of our models on the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error \n",
    "import numpy as np\n",
    "\n",
    "def regression_performance(math_train_vars, math_train_score, math_test_vars, math_test_score, pipeline):\n",
    "\tprint(\"Model Evaluation \\n\")\n",
    "\tprint(\"* Train Set\")\n",
    "\tregression_evaluation(math_train_vars, math_train_score, pipeline)\n",
    "\tprint(\"* Test Set\")\n",
    "\tregression_evaluation(math_test_vars, math_test_score, pipeline)\n",
    "\n",
    "def regression_evaluation(X, y, pipeline):\n",
    "  prediction = pipeline.predict(X)\n",
    "  print('R2 Score:', r2_score(y, prediction).round(3))  \n",
    "  print('Mean Absolute Error:', mean_absolute_error(y, prediction).round(3))  \n",
    "  print('Mean Squared Error:', mean_squared_error(y, prediction).round(3))  \n",
    "  print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y, prediction)).round(3))\n",
    "  print(\"\\n\")\n",
    "\n",
    "\n",
    "def regression_evaluation_plots(math_train_vars, math_train_score, math_test_vars, math_test_score, pipeline, alpha_scatter=0.5):\n",
    "  pred_train = pipeline.predict(math_train_vars)\n",
    "  pred_test = pipeline.predict(math_test_vars)\n",
    "\n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\n",
    "  sns.scatterplot(x=math_train_score , y=pred_train, alpha=alpha_scatter, ax=axes[0])\n",
    "  sns.lineplot(x=math_train_score , y=math_train_score, color='red', ax=axes[0])\n",
    "  axes[0].set_xlabel(\"Actual\")\n",
    "  axes[0].set_ylabel(\"Predictions\")\n",
    "  axes[0].set_title(\"Train Set\")\n",
    "\n",
    "  sns.scatterplot(x=math_test_score , y=pred_test, alpha=alpha_scatter, ax=axes[1])\n",
    "  sns.lineplot(x=math_test_score , y=math_test_score, color='red', ax=axes[1])\n",
    "  axes[1].set_xlabel(\"Actual\")\n",
    "  axes[1].set_ylabel(\"Predictions\")\n",
    "  axes[1].set_title(\"Test Set\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the functions for the Linear Regression algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(math_train_vars, math_train_score, math_test_vars, math_test_score, best_regressor_pipeline_1)\n",
    "regression_evaluation_plots(math_train_vars, math_train_score, math_test_vars, math_test_score, best_regressor_pipeline_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the function call for the RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_performance(math_train_vars, math_train_score, math_test_vars, math_test_score, best_regressor_pipeline_2)\n",
    "regression_evaluation_plots(math_train_vars, math_train_score, math_test_vars, math_test_score, best_regressor_pipeline_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForestRegressor appears to offer somewhat improved performance, but the R2 scores for both algorithms are poor.\n",
    "\n",
    "As in the Churnometer project, we could try a Principal Component Analysis. However, we only have 5 feature variables, so this is unlikely to sufficiently improve performance. Instead, we will move straight to converting this task from regression to classification. To clearly separate matters, we will explore this classification task in the next notebook - 08b-math-score-model-classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11 (default, Nov  9 2022, 10:36:22) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03672d1d5ffbbadc3038e50358243ff6e1c939c705228639babab7898b4f75b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
