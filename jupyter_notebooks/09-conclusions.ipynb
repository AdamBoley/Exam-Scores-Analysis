{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Round up our data analysis and machine learning model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- Classification reports and confusion matrices of all final 2-bin models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We have now trained our 3 machine learning pipelines for predicting a student's math score, reading score and writing score. For the sake of completeness, we will round up the performance of these pipelines in one place, so as to evaluate performance side-by-side.\n",
    "\n",
    "### Terminology\n",
    "- Recall is the percentage of a particular class that was properly predicted\n",
    "- Precision is the percentage of predicted results related to a particular class that were actually correct\n",
    "\n",
    "### Math Score\n",
    "\n",
    "The math score pipeline required a classification task, after a regression task failed to perform well. We identified the lunch_program, ethnicity and parental_education variables as being the most important, and when the other variables were eliminated, we saw no loss of performance.\n",
    "\n",
    "Below is the classification report and confusion matrix for the final math_score classification model, copied wholesale from that notebook, and transposed into markdown tables:\n",
    "\n",
    "**Train Set**\n",
    "\n",
    "| Confusion Matrix    |                      |                               |\n",
    "|---------------------|----------------------|-------------------------------|\n",
    "|                     | Actual below average | Actual better than average    |\n",
    "| Prediction <66.5    |         263          |            110                |\n",
    "| Prediction >66.4    |        125           |          302                  |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| below average         |     0.71    |  0.68           |    0.69      |    388   |\n",
    "| better than average   |     0.71    |  0.73           | 0.72         |     412  |\n",
    "| accuracy              |             |                 |  0.71        | 800      |\n",
    "| macro avg             | 0.71        | 0.71            |  0.71        | 800      |\n",
    "| weighted avg          | 0.71        | 0.71            | 0.71         | 800      |\n",
    " \n",
    "<br>\n",
    "\n",
    "The key figures here are the recall score on the first class - 68%, and the precision on that same class - 71%. Given that our stated task to identify as many students who will likely underperform as possible, my determination is that recall is the most important score. This determination was aided by [this Medium article](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2). In particular, the paragraph that begins 'What is more important, precision or recall?' was useful. The author uses the example of a classifier built to detect patients with diabetes. In that case, the classifier needs to be able to correctly identify diabetics, and therefore a high recall score is needed. If we subsitute underperforming students for diabetics, then the same logic applies. This means that 68% (263 out of 388) of students likely to underperform were correctly identified.\n",
    "\n",
    "\n",
    "**Test Set**                                       \n",
    "\n",
    "| Confusion Matrix                  |                     |                               |\n",
    "|-----------------------------------|---------------------|-------------------------------|\n",
    "|                                   |Actual below average | Actual better than average    |\n",
    "| Prediction below average          |     57              |      37                       |\n",
    "| Prediction  better than average   |    55               |     51                        |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| below average         |   0.61      |   0.51          |  0.55        |   112    |\n",
    "| better than average   | 0.48        |     0.58        |   0.53       |  88      |\n",
    "| accuracy              |             |                 |    0.54      |   200    |\n",
    "| macro avg             |    0.54     |   0.54          |   0.54       |   200    |\n",
    "| weighted avg          |    0.55     |  0.54           |  0.54        |    200   |\n",
    "\n",
    "If we examine the Test Set results, we see a recall score of 51% on the lower-performing class. This matches what we would normally expect to see in a model, where performance on the train set is higher than on the test set. Recall scores of 68% and 51% are not great, but neither are they poor. From here on, I will say that performance is *decent*, as a shorthand for not great and not poor.\n",
    "<br>\n",
    "We also do not see excellent performance on the train set and poor performance on the test set, which would indicate overfitting. Therefore, we can say that the math score pipeline is likely underfit slightly. The Code Institute Predictive Analytics notebooks indicates that underfitting can be caused by:\n",
    "\n",
    "- a too-small dataset\n",
    "- poor algorithm selection\n",
    "- insufficiently informative feature variables\n",
    "- a too-small number of features\n",
    "- ineffective hyperparameters\n",
    "\n",
    "In the math score notebook, we took pains to select the correct algorithm and optimise the hyperparameters, and we also used a feature selection step in our exploratory pipeline to determine the relevant features, so these are clearly not contributory factors. Our dataset only consists of 1000 records, so we must conclude that the decent performance of the model is due to the small dataset and that the dataset's feature variables are not overly informative. That said, the dataset's variables may actually be informative, since the ethnicity variable has 5 possible values, and the parental education variable has 6 possible values. Combined with the binary lunch_program variable, we have 60 possible combinations (6 x 5 x 2)\n",
    "\n",
    "### Reading score\n",
    "\n",
    "The reading score pipeline required a classfication task, which we used from the outset, as we predicted that a regression model would perform poorly, given the problems we had with the math score regression model. We identified the lunch_program and test_preparation_course variables as being the most important. As above, the classification report and confusion matrix for the final model is below, transposed exactly and rendered into markdown tables:\n",
    "\n",
    "**Train Set**\n",
    "\n",
    "| Confusion Matrix                      |                      |                            |\n",
    "|---------------------------------------|----------------------|----------------------------|\n",
    "|                                       | Actual below average | Actual average or above    |\n",
    "| Prediction below average              |         360          |            256             |\n",
    "| Prediction actual average or above    |         50           |           134              |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| below average         |     0.58    |  0.88           |    0.70      |    410   |\n",
    "| average or better     |     0.73    |  0.34           | 0.47         |     390  |\n",
    "| accuracy              |             |                 |  0.62        | 800      |\n",
    "| macro avg             | 0.66        | 0.61            |  0.58        | 800      |\n",
    "| weighted avg          | 0.65        | 0.62            | 0.59         | 800      |\n",
    "\n",
    "As above, the most important metrics are the recall score on the lower peforming class - 0.88, and the precision score on that same class - 0.58. Per the reasoning above, the recall score is the more important metric. The recall score of 0.88 is much better than on the train set of the math score pipeline, and is in fact excellent. A score of 0.88 means that 88% of students who underperform were correctly identified.\n",
    "\n",
    "The recall performance of 0.34 on the higher-performing class is poor. However, as noted in the reading score notebook, this is immaterial, since the business requirements call for predictive performance on the lowest scoring class to be as high as possible.\n",
    "\n",
    "\n",
    "**Test Set**                                       \n",
    "\n",
    "| Confusion Matrix                |                      |                   |\n",
    "|---------------------------------|----------------------|-------------------|\n",
    "|                                 | Actual below average | average or better |\n",
    "| Prediction below average        |     91               |      68           |\n",
    "| Prediction average or better    |    16                |     25            |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| below average         |   0.57      |   0.85          |  0.68        |  107     |\n",
    "| average or better     | 0.61        |     0.27        |   0.37       |  93      |\n",
    "| accuracy              |             |                 |    0.58      |   200    |\n",
    "| macro avg             |    0.59     |   0.56          |   0.53       |   200    |\n",
    "| weighted avg          |    0.59     |  0.58           |  0.54        |    200   |\n",
    "\n",
    "The recall score on the test set is 0.85, which is slightly less than that of the train set. This is perfectly in line with expected behaviour, and the high recall score indicates normal fitting behaviour, so underfitting and overfitting are not present.\n",
    "\n",
    "\n",
    "### Writing Score\n",
    "\n",
    "The writing score pipeline required a classfication task, which we used from the outset, as we predicted that a regression model would perform poorly, given the problems we had with the math score regression model. As with the math score and reading score pipelines, we identified the lunch_program, test_preparation_course and parental_education variables as being the most important. As above, the classification report and confusion matrix for the final model is below, transposed exactly and rendered into markdown tables:\n",
    "\n",
    "**Train Set**\n",
    "\n",
    "| Confusion Matrix                  |                        |                               |\n",
    "|-----------------------------------|------------------------|-------------------------------|\n",
    "|                                   |Actual average or below | Actual better than average    |\n",
    "| Prediction average or below       |         403            |            333                |\n",
    "| Prediction better than average    |         7              |           57                  |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| average or below      |     0.55    |  0.98           |    0.70      |    410   |\n",
    "| better than average   |     0.89    |  0.15           | 0.25         |     390  |\n",
    "| accuracy              |             |                 |  0.57        | 800      |\n",
    "| macro avg             | 0.72        | 0.56            |  0.48        | 800      |\n",
    "| weighted avg          | 0.71        | 0.57            | 0.48         | 800      |\n",
    "\n",
    "\n",
    "As above, the most important metric here is the recall score of 0.98. This is excellent performance, indicative of near-perfect predictive performance. As with the reading score pipeline, recall performance on the higher-performing class is poor, which is not ideal, but is acceptable given the stated objective of the project.\n",
    "\n",
    "\n",
    "**Test Set**                                       \n",
    "\n",
    "| Confusion Matrix                  |                        |                               |\n",
    "|-----------------------------------|------------------------|-------------------------------|\n",
    "|                                   |Actual average or below | Actual better than average    |\n",
    "| Prediction average or below       |     101                |      83                       |\n",
    "| Prediction better than average    |    4                   |     12                        |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| average or below      |   0.55      |   0.96          |  0.70        |  105     |\n",
    "| better than average   | 0.75        |     0.13        |   0.22       |  95      |\n",
    "| accuracy              |             |                 |    0.56      |   200    |\n",
    "| macro avg             |    0.65     |   0.54          |   0.46       |   200    |\n",
    "| weighted avg          |    0.64     |  0.56           |  0.47        |    200   |\n",
    "\n",
    "The recall score of the test set is slightly lower at 0.96, but is still excellent.\n",
    "\n",
    "### Final conclusions\n",
    "\n",
    "Our models are quite different from one another. The math_score model has the lowest performance, but we predicted that this might be the case when we noted that, in the standard dataset, it has high skew and kurtosis coefficients. That said, the math_score pipeline's performance is not poor, and is acceptable. The math_score model was trained on the lunch_program, ethnicity and parental_education variables.\n",
    "\n",
    "The reading_score pipelines has much better performance, with very high recall scores for the lower class that indicates excellent predictive performance for that class. The reading_score model was trained on the lunch_program and test_preparation_course variables.\n",
    "\n",
    "The writing_score pipelines has even better performance, with exceptionally high recall scores for the lower class that indicates near-perfect predictive performance for that class. The writing_score model was trained on the lunch_program, test_preparation_course and parental_education variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11 (default, Nov  9 2022, 10:36:22) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03672d1d5ffbbadc3038e50358243ff6e1c939c705228639babab7898b4f75b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
