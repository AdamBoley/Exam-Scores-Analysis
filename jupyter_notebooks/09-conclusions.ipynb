{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Conclusions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Round up our data analysis and machine learning model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- all machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "- Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We have now trained our 3 machine learning pipelines for predicting a student's math score, reading score and writing score. For the sake of completeness, we will round up the performance of these pipelines in one place, so as to evaluate performance side-by-side.\n",
    "\n",
    "### Terminology\n",
    "- Recall is the percentage of a particular class that was properly predicted\n",
    "- Precision is the percentage of predicted results related to a particular class that were actually correct\n",
    "\n",
    "### Math Score\n",
    "\n",
    "The math score pipeline required a classification task, after a regression task failed to perform well. We identified the ethnicity and parental_education variables as being the most important, and when the other variables were eliminated, we saw no loss of performance.\n",
    "\n",
    "Below is the classification report and confusion matrix for the final math_score classification model, copied wholesale from that notebook, and transposed into markdown tables:\n",
    "\n",
    "**Train Set**\n",
    "\n",
    "| Confusion Matrix    |             |                 |              |\n",
    "|---------------------|-------------|-----------------|--------------|\n",
    "|                     |Actual <59.0 | Actual 59 to 74 | Actual >74.0 |\n",
    "| Prediction <59.0    |         160 |            105  |         65   |\n",
    "| Prediction 59 to 74 |         65  |          115    |       84     |\n",
    "| Prediction >74.0    |         32  |           62    |      112     |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| <59.0                 |     0.48    |  0.62           |    0.55      |    257   |\n",
    "| 59 to 74              |     0.44    |  0.41           |  0.42        |    282   |\n",
    "| >74.0                 |     0.54    |  0.43           | 0.48         |     261  |\n",
    "| accuracy              |             |                 |  0.48        | 800      |\n",
    "| macro avg             | 0.49        | 0.49            |  0.48        | 800      |\n",
    "| weighted avg          | 0.49        | 0.48            | 0.48         | 800      |\n",
    " \n",
    "<br>\n",
    "\n",
    "The key figures here are the recall score on the less than 59 class - 62%, and the precision on that same class - 48%. Given that our stated task to identify as many students who will likely underperform as possible, my determination is that recall is the most important score. This determination was aided by [this Medium article](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2). In particular, the paragraph that begins 'What is more important, precision or recall?' was useful. The author uses the example of a classifier built to detect patients with diabetes. In that case, the classifier needs to be able to correctly identify diabetics, and therefore a high recall score is needed. If we subsitute underperforming students for diabetics, then the same logic applies. This means that 62% (160 out of 257) of students likely to underperform were correctly identified. In addition 25% (65 out of 257) of the students who actually underperform were predicted to fall into the middling performance class, so they will still receive some help.\n",
    "\n",
    "\n",
    "**Test Set**                                       \n",
    "\n",
    "| Confusion Matrix    |             |                 |              |\n",
    "|---------------------|-------------|-----------------|--------------|\n",
    "|                     |Actual <59.0 | Actual 59 to 74 | Actual >74.0 |\n",
    "| Prediction <59.0    |     49      |      28         |     13       |\n",
    "| Prediction  59 to 74|   16        |    21           |    21        |\n",
    "| Prediction  >74.0   |    13       |     18          |     21       |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| <59.0                 |   0.54      |   0.63          |  0.58        |   78     |\n",
    "| 59 to 74              | 0.36        |    0.31         |    0.34      |   67     |\n",
    "| >74.0                 | 0.40        |     0.38        |   0.39       |  55      |\n",
    "| accuracy              |             |                 |        0.46  |   200    |\n",
    "| macro avg             |    0.44     |   0.44          |   0.44       |   200    |\n",
    "| weighted avg          |    0.44     |  0.46           |  0.45        |    200   |\n",
    "\n",
    "If we examine the Test Set results, we see a recall score of 63% on the lowest-performing class. This matches the recall score of the Train set. Normally, we would expect performance on the train set to be slightly higher than on the test set. Recall scores of 62% and 63% are not great, but neither are they poor. From here on, I will say that performance is *decent*, as a shorthand for not great and not poor.\n",
    "<br>\n",
    "We also do not see excellent performance on the train set and poor performance on the test set, which might indicate overfitting. Therefore, we can say that the math score pipeline is likely underfit slightly. The Code Institute Predictive Analytics notebooks indicates that underfitting can be caused by:\n",
    "\n",
    "- a too-small dataset\n",
    "- poor algorithm selection\n",
    "- insufficiently informative feature variables\n",
    "- a too-small number of features\n",
    "- ineffective hyperparameters\n",
    "\n",
    "In the math score notebook, we took pains to select the correct algorithm and optimise the hyperparameters, and we also used a feature selection step of our first pipeline to determine the relevant features, so these are clearly not contributory factors. Our dataset only consists of 1000 records, so we must conclude that the decent performance of the model is due to the small dataset and that the dataset's feature variables are not overly informative. That said, the dataset's variables may actually be informative, since the ethnicity variable has 5 possible values, and the parental education variable has 6 possible values.\n",
    "\n",
    "### Reading score\n",
    "\n",
    "The reading score pipeline required a classfication task, which we used from the outset, as we predicted that a regression model would perform poorly, given the problems we had with the math score regression model. As with the math score pipeline, we identified the ethnicity and parental_education variables as being the most important. As above, the classification report and confusion matrix for the final model is below, transposed exactly and rendered into markdown tables:\n",
    "\n",
    "**Train Set**\n",
    "\n",
    "| Confusion Matrix    |             |                 |              |\n",
    "|---------------------|-------------|-----------------|--------------|\n",
    "|                     |Actual <63.0 | Actual 63 to 76 | Actual >76.0 |\n",
    "| Prediction <63.0    |         186 |            151  |        126   |\n",
    "| Prediction 63 to 76 |         31  |          44     |       36     |\n",
    "| Prediction >76.0    |         61  |           70    |      95      |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| <63.0                 |     0.40    |  0.67           |    0.50      |    278   |\n",
    "| 63 to 76              |     0.40    |  0.17           |  0.23        |    265   |\n",
    "| >76.0                 |     0.42    |  0.37           | 0.39         |     257  |\n",
    "| accuracy              |             |                 |  0.41        | 800      |\n",
    "| macro avg             | 0.41        | 0.40            |  0.38        | 800      |\n",
    "| weighted avg          | 0.41        | 0.41            | 0.38         | 800      |\n",
    "\n",
    "As above, the most important metrics are the recall score on the lowest peforming class - 0.67, and the precision score on that same class - 0.40. Per the reasoning above, the recall score is the more important metric. The recall score of 0.67 is slightly better than on the train set of the math score pipeline, but is still not great, though again, neither is it poor. It is merely decent. A score of 0.67 means that 67% of students who underperform were correctly identified.\n",
    "\n",
    "However, where the math score recall on the other 2 classes was around 0.40, which might be called OK, the only reading score recall on the best performing class is at this level, with performance on the middle class being poor, at 0.17. Ideally, we would want this to be higher, so that we could have at least some confidence in the ability of the model to predict all classes, not just the lowest performing one.\n",
    "\n",
    "\n",
    "**Test Set**                                       \n",
    "\n",
    "| Confusion Matrix    |             |                 |              |\n",
    "|---------------------|-------------|-----------------|--------------|\n",
    "|                     |Actual <63.0 | Actual 63 to 76 | Actual >76.0 |\n",
    "| Prediction <63.0    |     53      |      36         |     25       |\n",
    "| Prediction 63 to 76 |   12        |    11           |    9         |\n",
    "| Prediction >74.0    |    13       |     15          |     26       |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| <63.0                 |   0.46      |   0.68          |  0.55        |   78     |\n",
    "| 63 to 76              | 0.34        |    0.18         |    0.23      |   62     |\n",
    "| >76.0                 | 0.48        |     0.43        |   0.46       |  60      |\n",
    "| accuracy              |             |                 |    0.45      |   200    |\n",
    "| macro avg             |    0.43     |   0.43          |   0.41       |   200    |\n",
    "| weighted avg          |    0.43     |  0.45           |  0.42        |    200   |\n",
    "\n",
    "The recall score on the test set is 0.68, which matches that of the train set. As before and for the same reasons as above, this could indicate that the model is slightly underfit, although to a lesser degree than with the math score. Overfitting is not present.\n",
    "\n",
    "\n",
    "### Writing Score\n",
    "\n",
    "The writing score pipeline required a classfication task, which we used from the outset, as we predicted that a regression model would perform poorly, given the problems we had with the math score regression model. As with the math score and reading score pipelines, we identified the ethnicity and parental_education variables as being the most important. As above, the classification report and confusion matrix for the final model is below, transposed exactly and rendered into markdown tables:\n",
    "\n",
    "**Train Set**\n",
    "\n",
    "| Confusion Matrix    |             |                 |              |\n",
    "|---------------------|-------------|-----------------|--------------|\n",
    "|                     |Actual <62.0 | Actual 62 to 75 | Actual >75.0 |\n",
    "| Prediction <62.0    |         189 |            150  |        104   |\n",
    "| Prediction 62 to 75 |         20  |          37     |       22     |\n",
    "| Prediction >75.0    |         70  |           83    |      125     |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| <62.0                 |     0.43    |  0.68           |    0.52      |    279   |\n",
    "| 62 to 75              |     0.47    |  0.14           |  0.21        |    270   |\n",
    "| >75.0                 |     0.45    |  0.50           | 0.47         |     251  |\n",
    "| accuracy              |             |                 |  0.44        | 800      |\n",
    "| macro avg             | 0.45        | 0.44            |  0.40        | 800      |\n",
    "| weighted avg          | 0.45        | 0.44            | 0.40         | 800      |\n",
    "\n",
    "\n",
    "As above, the most important metric here is the recall score of 0.68. This recall score matches the recall score of the train set of the reading score pipeline. Again, this is decent performance. As with the reading score pipeline, recall performance on the middle class is poor, which is not ideal, but is acceptable given the stated objective of the project.\n",
    "\n",
    "\n",
    "**Test Set**                                       \n",
    "\n",
    "| Confusion Matrix    |             |                 |              |\n",
    "|---------------------|-------------|-----------------|--------------|\n",
    "|                     |Actual <62.0 | Actual 62 to 75 | Actual >75.0 |\n",
    "| Prediction <62.0    |     55      |      30         |     27       |\n",
    "| Prediction 62 to 75 |    6        |     6           |    8         |\n",
    "| Prediction >75.0    |    17       |     21          |     30       |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "| Classification Report |             |                 |              |          |\n",
    "|-----------------------|-------------|-----------------|--------------|----------|\n",
    "|                       | precision   | recall          | f1-score     |  support |\n",
    "| <62.0                 |   0.46      |   0.71          |  0.58        |   78     |\n",
    "| 62 to 75              | 0.30        |    0.11         |    0.16      |   57     |\n",
    "| >75.0                 | 0.44        |     0.46        |   0.45       |  65      |\n",
    "| accuracy              |             |                 |    0.46      |   200    |\n",
    "| macro avg             |    0.41     |   0.42          |   0.40       |   200    |\n",
    "| weighted avg          |    0.42     |  0.46           |  0.42        |    200   |\n",
    "\n",
    "The recall score of the test set is slightly higher than we might expect at 0.71. With the other 2 pipelines, the recall score of the train and test sets are nearly identical. Here, the difference is a noticeable 3 percentage points. As above, the merely decent performance of the writing score model indicates that it is perhaps slightly underfit, but certainly not overfit.\n",
    "\n",
    "### Final conclusions\n",
    "\n",
    "All three of our models are quite similar. All three use the ethnicity and parental_education variables, and all have recall scores for the lowest performing class between 0.60 and 0.70, for both the train and test sets, making the performance of all 3 models decent - a shorthand for neither great not poor. All three pipelines are multiple classification models with 3 classes. The break-points (where each class starts and stops) and widths of those classes are reasonably consistent across the models.\n",
    "\n",
    "The consistences of the models is a bonus, and should simplify their use within the Streamlit dashboard - users will only need to select options for 2 variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11 (default, Nov  9 2022, 10:36:22) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03672d1d5ffbbadc3038e50358243ff6e1c939c705228639babab7898b4f75b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
